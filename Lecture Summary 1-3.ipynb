{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lecture Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Mr. Smith, how are you doing today?', 'The weather is great, and Python is awesome.', 'The sky is pinkish-blue.', \"You shouldn't eat so many cookies.\"]\n",
      "['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', ',', 'and', 'Python', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'You', 'should', \"n't\", 'eat', 'so', 'many', 'cookies', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "TEXT = \"Hello Mr. Smith, how are you doing today? The weather is great, and Python is awesome. The sky is pinkish-blue. You shouldn't eat so many cookies.\"\n",
    "print(sent_tokenize(TEXT))\n",
    "print(word_tokenize(TEXT))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Part-of-speech (POS) tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('And', 'CC'), ('now', 'RB'), ('for', 'IN'), ('something', 'NN'), ('completely', 'RB'), ('different', 'JJ')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "text=word_tokenize(\"And now for something completely different\")\n",
    "print nltk.pos_tag(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Unigrams Vs Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({('camera', 'produces'): 1, ('This', 'camera'): 1, ('awesome', 'pictures'): 1, ('produces', 'awesome'): 1, ('pictures', '.'): 1})\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "text = \"This camera produces awesome pictures.\"\n",
    "token = nltk.word_tokenize(text)\n",
    "bigrams = ngrams(token,2)\n",
    "trigrams = ngrams(token,3)\n",
    "print Counter(bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Stemming & Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming:  “The process for reducing inflected (or sometimes derived) words to their stem…”\n",
    "\n",
    "Lemmatization: “The process of grouping together the different inflected forms of a word so they can be analyzed as a single item.”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', u'stripe', u'bat', 'are', u'hang', 'on', 'their', 'feet', 'for', 'best']\n"
     ]
    }
   ],
   "source": [
    "token = nltk.word_tokenize(\"The striped bats are hanging on their feet for best\")\n",
    "from nltk.stem import PorterStemmer\n",
    "print [PorterStemmer().stem(t) for t in token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'striped', u'bat', 'are', 'hanging', 'on', 'their', u'foot', 'for', 'best']\n"
     ]
    }
   ],
   "source": [
    "wnl = nltk.WordNetLemmatizer()\n",
    "print [wnl.lemmatize(t) for t in token]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term Frequency (tf) is the frequency of j th term in the i th document ($f_{ij}$)\n",
    "\n",
    "Inverse document frequency (idf) is document freqeuncy of the j th term (what % of documents have the term)\n",
    "\n",
    "Term frequency-Inverse document frequency (tf-idf) = $f_{ij}$ * log(1/ $F_{j}$)\n",
    "\n",
    "The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "corpus = ['This is the first document.', \n",
    "          'This document is the second document.',\n",
    "          'And this is the third one.',\n",
    "          'Is this the first document?']\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>TF-IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and</td>\n",
       "      <td>0.127962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>document</td>\n",
       "      <td>0.406802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>first</td>\n",
       "      <td>0.290143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>is</td>\n",
       "      <td>0.329091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>one</td>\n",
       "      <td>0.127962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>second</td>\n",
       "      <td>0.134662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>the</td>\n",
       "      <td>0.329091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>third</td>\n",
       "      <td>0.127962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>this</td>\n",
       "      <td>0.329091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Term    TF-IDF\n",
       "0       and  0.127962\n",
       "1  document  0.406802\n",
       "2     first  0.290143\n",
       "3        is  0.329091\n",
       "4       one  0.127962\n",
       "5    second  0.134662\n",
       "6       the  0.329091\n",
       "7     third  0.127962\n",
       "8      this  0.329091"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = np.asarray(X.mean(axis=0)).ravel()\n",
    "weights_df = pd.DataFrame({'Term':vectorizer.get_feature_names(), 'TF-IDF': weights})\n",
    "weights_df = weights_df[['Term', 'TF-IDF']]\n",
    "weights_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zipf's Law: Frequency of a word is inversely proportional to its rank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Naive Bayes i Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naïve Bayes makes two simplifying assumptions: \n",
    "\n",
    "    1. probability of each word occurring in the document is independent of the occurrences of the other words;\n",
    "    \n",
    "    2. The position of words in the document does not matter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes'Rules applied to Documents and Classes \n",
    "\n",
    "$P ( c|d ) = [ P( d | c ) * P( c ) ] / [ P( d ) ]$\n",
    "\n",
    "where P(c) is the total probability of a class, in other words, how often does this class occur in total?\n",
    "\n",
    "$P( d | c )$ is the probability of word i happened in class j "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One problem is that, say, the word 'fantastic' is not in the traning documents in class positive. Then the P(fantastic | positive) = 0 and by multiplying other terms, the whole probability will be zero. To fix this, we use a Smoothing Algorithm (Laplace Smoothing): we add one to both the denominator and numerator. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example found here: https://streamhacker.com/2010/05/10/text-classification-sentiment-analysis-naive-bayes-classifier/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this steps creates a bag of words. \n",
    "def word_feats(words):\n",
    "        return dict([(word, True) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 1500 instances, test on 500 instances\n",
      "accuracy: 0.728\n",
      "Most Informative Features\n",
      "             magnificent = True              pos : neg    =     15.0 : 1.0\n",
      "             outstanding = True              pos : neg    =     13.6 : 1.0\n",
      "               insulting = True              neg : pos    =     13.0 : 1.0\n",
      "              vulnerable = True              pos : neg    =     12.3 : 1.0\n",
      "               ludicrous = True              neg : pos    =     11.8 : 1.0\n",
      "                  avoids = True              pos : neg    =     11.7 : 1.0\n",
      "             uninvolving = True              neg : pos    =     11.7 : 1.0\n",
      "              astounding = True              pos : neg    =     10.3 : 1.0\n",
      "             fascination = True              pos : neg    =     10.3 : 1.0\n",
      "                 idiotic = True              neg : pos    =      9.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "negids = movie_reviews.fileids('neg')\n",
    "posids = movie_reviews.fileids('pos')\n",
    " \n",
    "negfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'neg') for f in negids]\n",
    "posfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'pos') for f in posids]\n",
    " \n",
    "negcutoff = len(negfeats)*3/4\n",
    "poscutoff = len(posfeats)*3/4\n",
    " \n",
    "trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
    "testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    "print 'train on %d instances, test on %d instances' % (len(trainfeats), len(testfeats))\n",
    " \n",
    "classifier = NaiveBayesClassifier.train(trainfeats)\n",
    "print 'accuracy:', nltk.classify.util.accuracy(classifier, testfeats)\n",
    "classifier.show_most_informative_features()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
